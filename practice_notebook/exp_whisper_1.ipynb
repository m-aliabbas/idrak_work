{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing the Libs"
      ],
      "metadata": {
        "id": "skhQiREcUdDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e_LgAoH84wY",
        "outputId": "2af5a4d8-d743-4c77-ca21-560d749ca537"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 58.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
            "\u001b[K     |████████████████████████████████| 452 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 59.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 60.9 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 32.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.8.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libs"
      ],
      "metadata": {
        "id": "HjOU2FG2Uglm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "44OH82ZNA7vS"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio"
      ],
      "metadata": {
        "id": "0PoJ3yB9I671"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WhisperTranscriptorAPI Defination**\n"
      ],
      "metadata": {
        "id": "OcaCRedLUpbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WhisperTranscriptorAPI:\n",
        "    '''\n",
        "\n",
        "    This Module is based on OpenAI Whisper for Audio Transcription.\n",
        "    We need WhisperProcessor and WhisperConditionalGeneration for \n",
        "    CTC task i.e. ASR. \n",
        "\n",
        "    example:\n",
        "          whisper_transcriptor=WhisperTranscriptorAPI(model_path='openai/whisper-tiny.en')\n",
        "          \n",
        "    '''\n",
        "    #----------------------- constructor -------------------------------------\n",
        "    #\n",
        "    def __init__(self,model_path=''):\n",
        "\n",
        "        '''\n",
        "\n",
        "        1) Defining processor for processing audio input for Whisper and\n",
        "        generate Pytorch token\n",
        "\n",
        "        2) Put Processed Audio to model and get PyTorch Tensor later this\n",
        "        tensor will be post processed by Lang Model.\n",
        "\n",
        "        args:\n",
        "          model_path: the huggingface repo of whisper-model. \n",
        "          ... i.e. for example: openai/whisper-tiny.en \n",
        "\n",
        "\n",
        "        '''\n",
        "\n",
        "        self.model_path = model_path\n",
        "        self.processor = WhisperProcessor.from_pretrained(self.model_path) \n",
        "        self.model = WhisperForConditionalGeneration.from_pretrained(self.model_path)\n",
        "        self.OUTPUT_DIR= \"temp\"\n",
        "\n",
        "\n",
        "    #--------------------------- convert mp3 to wav ---------------------------\n",
        "    #\n",
        "\n",
        "    def save_fn(self,filepath=''):\n",
        "        '''\n",
        "\n",
        "        If file is mp3 first convert it to wav.\n",
        "        \n",
        "        1) Read audio segment from Path\n",
        "        2) Sample the audio to 16kHZ\n",
        "        3) Save file in temp\n",
        "\n",
        "\n",
        "        args:\n",
        "            filepath(str): path of audio file example audio.mp3\n",
        "\n",
        "        returns:\n",
        "            path of temporary generated wav file\n",
        "        '''\n",
        "\n",
        "        path =filepath\n",
        "        filename=os.path.basename(filepath) #Getting filename from path\n",
        "        save_path = f\"{self.OUTPUT_DIR}\"\n",
        "        if not os.path.exists(save_path): #If path not exist create a directory\n",
        "            os.makedirs(save_path, exist_ok=True)\n",
        "        if os.path.exists(path):\n",
        "            try:\n",
        "                sound = AudioSegment.from_mp3(path) #Read a sound file\n",
        "                sound = sound.set_frame_rate(16000) #resample to 16000\n",
        "                sound.export(f\"{save_path}/{filename[:-4]}.wav\", format=\"wav\") #save to *.wav\n",
        "                return f\"{save_path}/{filename[:-4]}.wav\"\n",
        "            except:\n",
        "                print(path)\n",
        "\n",
        "\n",
        "    #------------------------- genrate transcript -----------------------------\n",
        "    #\n",
        "    def genrate_transcript(self,audio_path=''):\n",
        "        \n",
        "        '''\n",
        "        Generate the transcript from audio file using Whisper\n",
        "\n",
        "        1) This function will read the .wav audio using torchaudio\n",
        "        2) Process the audio using WhisperProcessor\n",
        "        3) Generate the predicted token ids using Whisper Model\n",
        "        4) Decode the output using Language model\n",
        "\n",
        "        args: \n",
        "            audio_path (str) : path of wav file\n",
        "\n",
        "        returns:\n",
        "            transcription(str): generated script\n",
        "        '''\n",
        "        self.audio_path = audio_path\n",
        "        exten = os.path.splitext(self.audio_path)[1]\n",
        "        if exten == '.mp3':\n",
        "            self.audio_path = self.save_fn(self.audio_path)\n",
        "        wave,fs=torchaudio.load(self.audio_path)\n",
        "        #tensor to numpy\n",
        "        wave=wave.numpy()\n",
        "        if len(wave.shape)>=2:\n",
        "            wave=wave[0,:]\n",
        "        inputs = self.processor(wave, return_tensors=\"pt\")\n",
        "        input_features = inputs.input_features\n",
        "        generated_ids = self.model.generate(inputs=input_features)\n",
        "        #decode the transcript using language model from processor \n",
        "        transcription = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "        return transcription"
      ],
      "metadata": {
        "id": "dsyRJHfrJEod"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Initialization"
      ],
      "metadata": {
        "id": "yWXaNbcTUnt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_transcriptor=WhisperTranscriptorAPI(model_path='openai/whisper-tiny.en')"
      ],
      "metadata": {
        "id": "7mi1802GN5A7"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiments:"
      ],
      "metadata": {
        "id": "PTAvnv_YU7rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcript1=whisper_transcriptor.genrate_transcript(audio_path='audio4.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvfusJYVOwTD",
        "outputId": "89b6ccb2-d958-493b-aae2-adda448241be"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nvZRckPJPVBC",
        "outputId": "fa272d35-181f-4bf5-d09a-02634790bc72"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Dr. Mortimer looked at homes with an air of professional interest, and Sir Henry Baker'sville turned a pair of puzzled dark eyes upon me.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript2=whisper_transcriptor.genrate_transcript(audio_path='abc/audio5.wav')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3fz43cfPqd-",
        "outputId": "0461114f-b6bf-4b56-a3d2-692c5ddc4ca4"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x3DDMfMhPut6",
        "outputId": "597be63c-8563-4344-a945-034b7af6ca2f"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' When I was taking my walk on the same day I met Zawiski with Labradi. I did not try to avoid them.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript3=whisper_transcriptor.genrate_transcript(audio_path='ali.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDh_umJKP4dn",
        "outputId": "a4cb326f-fa71-4cfc-92cf-ba2b69bfc66d"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yIp0OjZBT1WA",
        "outputId": "d936767e-a1d1-4577-bda1-7181e44d8782"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' A guru is an expert who has devoted many years to study and contemplation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript4=whisper_transcriptor.genrate_transcript(audio_path='temp/ali.wav')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VAJY566T381",
        "outputId": "74b16e97-7c71-4bef-8a49-2f4d78426907"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "temp/ali.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CM4u2x0mUEeW",
        "outputId": "da1aa010-c0b0-4ca1-de32-6d0234ea0ba2"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' A guru is an expert who has devoted many years to study and contemplation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xM0FLvKPUHLC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}