{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#TinyBertInfernce API\n",
        "Idrak is using Tiny Bert Model for the classification of texts. TinyBert out perform the classical ML *(Random Forest, SGD, LightGBM, XGBoost, Linear Regression, and SVM)* as well as other Bert Varients *(MobileBert, Distilt Bert and Bert-Base-Uncased)* . Also, Tiny Bert is occupying 50 MB space on disk while other Bert Models are taking more than 400 MB space. \n",
        "Moreover, We have initialy 4 datasets and 5 classifier for each dataset. \n",
        "The detail of datasets are following\n",
        "1.   Actual Text (Human Written Data)\n",
        "2.   Data from Current Transcriptor\n",
        "3.   Data from Transcriptor with Decoder (Beam)\n",
        "4.   Data from Transcriptor without Decoder (Greedy)\n",
        "\n",
        "\n",
        "And, the detail of classifier are following\n",
        "\n",
        "1.  Hello, (Answering Machine and DNC ) `NUM_CLASSES=2`\n",
        "2.  Intro, (Answering Machine, Busy, DNC, Greetings, sorry greetings, Greet back, Spanish, Other) `NUM_CLASSES=8`\n",
        "3.  Pitch, (Busy, DNC, Spanish, Other, Not interested, Positive, Negative, BOT) `NUM_CLASSES=8`\n",
        "4.  Yes No without Age Sheet (Positive, Negative, DNC, Other, Not interested) `NUM_CLASSES=5`\n",
        "5.  Yes No with Age Sheet (Positive, Negative, DNC, Other, Not interested) `NUM_CLASSES=5`\n",
        "This API will take a text string and return a dictionary containing probabilities of different classes `prob` and class label `class`"
      ],
      "metadata": {
        "id": "V5jlIWULhAGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting the Google Drive . You can ommit this line if you are running envoirnment other than Google Drive.\n",
        "\n",
        "**Dataset:** https://drive.google.com/drive/folders/1YDvc7E_QYwlhaxMGxk0E7YI1tNeixgiF?usp=share_link \n",
        "\n",
        "==> data/datasetX/classifierX_train.csv \n",
        "\n",
        "==> data/datasetX/classifierX_train_aug.csv \n",
        "\n",
        "==> data/datasetX/classifierX_test.csv\n",
        "\n",
        "**Models:** https://drive.google.com/drive/folders/1RbchTgviRCxcQ1wjniX79mM9VtxWSJ_o?usp=share_link \n",
        "\n",
        "**Checkpoint Path:**\n",
        "\n",
        "==> model/tinybert_report/best_datasetX_classifier_X.ckpt\n",
        "\n",
        "**Model History**\n",
        "\n",
        "==> model/tinybert_report/datasetX_classifier_X_history.csv\n",
        "\n",
        "**Wrong Predictions Record**\n",
        "\n",
        "==> model/tinybert_report/datasetX_classifier_X_invalid_predictions.csv\n",
        "\n",
        "**Classification Report**\n",
        "\n",
        "==> model/tinybert_report/datasetX_classifier_X_report.json\n"
      ],
      "metadata": {
        "id": "ivTAW20HoEy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installation"
      ],
      "metadata": {
        "id": "5kvVWifyqcRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the required models"
      ],
      "metadata": {
        "id": "g-JzcOzZqYvC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faYR-Tr-ljuw",
        "outputId": "4268bc1f-8abc-4ff3-da55-cda8ea8fb2cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 36.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.8.1-py3-none-any.whl (798 kB)\n",
            "\u001b[K     |████████████████████████████████| 798 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting lightning-utilities==0.3.*\n",
            "  Downloading lightning_utilities-0.3.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.64.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.9.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.21.6)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.1.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2022.10.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.10.2-py3-none-any.whl (529 kB)\n",
            "\u001b[K     |████████████████████████████████| 529 kB 54.0 MB/s \n",
            "\u001b[?25hCollecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.50.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.4.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.19.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (2.14.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.38.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning) (3.2.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->lightning-utilities==0.3.*->pytorch_lightning) (2.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115940 sha256=4c40d99537dc17f6948c3232b31661e7086935f973ea8bb66145bed8b067515a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, torchmetrics, lightning-utilities, pytorch-lightning\n",
            "Successfully installed fire-0.4.0 lightning-utilities-0.3.0 pytorch-lightning-1.8.1 torchmetrics-0.10.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting Drive"
      ],
      "metadata": {
        "id": "_iaS0gBVqgIX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EhZFED9oxxy",
        "outputId": "2a07185f-2c70-4318-b1a5-53165ac5e2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('gdrive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the Modules"
      ],
      "metadata": {
        "id": "u0vBCWIkqvt9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "qluWlsFIlon7"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer,BertModel\n",
        "from transformers import AdamW\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics import F1Score\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping,ProgressBarBase\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import warnings\n",
        "from sklearn.utils.extmath import softmax\n",
        "from torchmetrics import Accuracy\n",
        "import re\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model's Classes Initializations"
      ],
      "metadata": {
        "id": "lCjssB3SqzTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class to disable classes progress bar. Because progress consume more execution time."
      ],
      "metadata": {
        "id": "zuwHK3dCq5JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LitProgressBar(ProgressBarBase):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()  # don't forget this :)\n",
        "        self.enable = False\n",
        "\n",
        "    def disable(self):\n",
        "        self.enable = False\n",
        "\n",
        "    def on_train_batch_end(self, trainer, pl_module, outputs, batch_idx):\n",
        "        super().on_train_batch_end(trainer, pl_module, outputs, batch_idx)  # don't forget this :)\n",
        "        percent = (self.train_batch_idx / self.total_train_batches) * 100\n",
        "        # sys.stdout.flush()\n",
        "        # sys.stdout.write(f'{percent:.01f} percent complete \\r')\n"
      ],
      "metadata": {
        "id": "C-EqepD1b-3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P77kRWMmlvwM"
      },
      "outputs": [],
      "source": [
        "# RANDOM_SEED = 321\n",
        "\n",
        "# sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "# HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "# sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "# rcParams['figure.figsize'] = 12, 8\n",
        "\n",
        "# pl.seed_everything(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1l56aoTHlwCJ"
      },
      "outputs": [],
      "source": [
        "#Disable Warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW80sBrPlxp4"
      },
      "outputs": [],
      "source": [
        "# model_path='prajjwal1/bert-tiny'\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch lighting Requires a Dataset consiting of dataframe for prediction\n",
        "So creating the dataset and dataloader"
      ],
      "metadata": {
        "id": "jdUMwXHmrk_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pytorch lighting Requires a Dataset consiting of dataframe for prediction\n",
        "#So creating the dataset and dataloader"
      ],
      "metadata": {
        "id": "9n55SKZGrZmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6acvrDxYmbUH"
      },
      "outputs": [],
      "source": [
        "class CallCenterDataset(Dataset):\n",
        "  '''\n",
        "  Dataset for Bert Processing\n",
        "  '''\n",
        "  def __init__(\n",
        "    self, \n",
        "    data: pd.DataFrame, \n",
        "    tokenizer: AutoTokenizer, \n",
        "    max_token_len: int = 40\n",
        "  ):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = data\n",
        "    self.max_token_len = max_token_len\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    data_row = self.data.iloc[index]\n",
        "    comment_text = data_row.cleaned_text\n",
        "    labels = data_row['class']\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      comment_text,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_token_len,\n",
        "      return_token_type_ids=False,\n",
        "      padding=\"max_length\",\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "    return dict(\n",
        "      comment_text=comment_text,\n",
        "      input_ids=encoding[\"input_ids\"].flatten(),\n",
        "      attention_mask=encoding[\"attention_mask\"].flatten(),\n",
        "      labels=labels\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVrIT7DCmvZB"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a85fopZmzAy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CallCenterTagger** is model class. That require the forward,prediction and other class function of PyTorch model. Also, it have loss and other requirements."
      ],
      "metadata": {
        "id": "nmV9INp0rszA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQXZ4BmPm4Z8"
      },
      "outputs": [],
      "source": [
        "class CallCenterTagger(pl.LightningModule):\n",
        "  '''\n",
        "  This is PyTorch model class having forward and other method requird for trianing,validation, testing and prediciton.\n",
        "  Arguments:\n",
        "  n_classes(int): Number of classes to predict\n",
        "  model_path(str): pretrained models path \n",
        "  '''\n",
        "  def __init__(self, n_classes: int,model_path=None,n_training_steps=None, n_warmup_steps=None,learning_rate=0.02):\n",
        "    super().__init__()\n",
        "\n",
        "    # return_dict=True\n",
        "    self.bert = BertModel.from_pretrained(model_path, return_dict=True)\n",
        "    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "       \n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.n_training_steps = n_training_steps\n",
        "    self.n_warmup_steps = n_warmup_steps\n",
        "     \n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    self.train_f1 = F1Score(num_classes=n_classes,average=\"micro\")\n",
        "    self.train_acc=Accuracy()\n",
        "    self.val_f1=F1Score(num_classes=n_classes,average=\"micro\")\n",
        "    self.val_acc=Accuracy()\n",
        "    self.learning_rate=learning_rate\n",
        "  def forward(self, input_ids, attention_mask, labels=None):\n",
        "    \n",
        "    output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "    last_state_output=output.last_hidden_state[:,0,:]\n",
        "   \n",
        "    output = self.classifier(last_state_output)\n",
        "\n",
        "    loss = 0\n",
        "    if labels is not None:\n",
        "        loss = self.criterion(output, labels)\n",
        "    return loss, output\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    input_ids = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "    labels = batch[\"labels\"]\n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    y_pred=outputs\n",
        "    acc = self.train_acc(y_pred, labels)\n",
        "    f1 = self.train_f1(y_pred, labels)\n",
        "\n",
        "    self.log(\"train_accuracy\", acc)\n",
        "    self.log(\"train_f1\", f1)\n",
        "    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    input_ids = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "    labels = batch[\"labels\"]\n",
        "    \n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    y_pred = outputs\n",
        "    acc=self.val_acc(y_pred,labels)\n",
        "    f1=self.val_f1(y_pred, labels)\n",
        "\n",
        "    self.log(\"valid_accuracy\", acc)\n",
        "    self.log(\"valid_f1\", f1)\n",
        "    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    input_ids = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "    labels = batch[\"labels\"]\n",
        "    \n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    y_pred = outputs\n",
        "    acc=self.val_acc(y_pred,labels)\n",
        "    f1=self.val_f1(y_pred, labels)\n",
        "\n",
        "    self.log(\"test_accuracy\", acc)\n",
        "    self.log(\"test_f1\", f1)\n",
        "    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
        "    return loss\n",
        "  def predict_step(self,batch,batch_idx):\n",
        "    input_ids = batch[\"input_ids\"]\n",
        "    attention_mask = batch[\"attention_mask\"]\n",
        "    labels = batch[\"labels\"]\n",
        "    \n",
        "    loss, outputs = self(input_ids, attention_mask, labels)\n",
        "    y_pred = outputs\n",
        "    acc=self.val_acc(y_pred,labels)\n",
        "    f1=self.val_f1(y_pred, labels)\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "  def training_epoch_end(self, outputs):\n",
        "    \n",
        "    labels = []\n",
        "    predictions = []\n",
        "    for output in outputs:\n",
        "      for out_labels in output[\"labels\"].detach().cpu():\n",
        "        labels.append(out_labels)\n",
        "      for out_predictions in output[\"predictions\"].detach().cpu():\n",
        "        predictions.append(out_predictions)\n",
        "\n",
        "    labels = torch.stack(labels).int()\n",
        "    predictions = torch.stack(predictions)\n",
        "    train_accuracy = self.train_acc.compute()\n",
        "    train_f1 = self.train_f1.compute()\n",
        "    print('Train Accuracy: ',train_accuracy)\n",
        "    print('Train F1: ',train_f1)\n",
        "    self.log(\"epoch_train_accuracy\", train_accuracy)\n",
        "    self.log(\"epoch_train_f1\", train_f1)\n",
        "    self.train_acc.reset()\n",
        "    self.train_f1.reset()\n",
        "\n",
        "\n",
        "  def validation_epoch_end(self, outputs):\n",
        "    val_accuracy = self.val_acc.compute()\n",
        "    val_f1 = self.val_f1.compute()\n",
        "    print('Valid Accuracy: ',val_accuracy)\n",
        "    print('Valid F1: ',val_f1)\n",
        "    # log metrics\n",
        "    self.log(\"epoch_val_accuracy\", val_accuracy)\n",
        "    self.log(\"epoch_val_f1\", val_f1)\n",
        "    self.val_acc.reset()\n",
        "    self.val_f1.reset()\n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "    LEARNING_RATE=self.learning_rate\n",
        "    param_optimizer = list(self.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.05},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.01}\n",
        "        ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)\n",
        "\n",
        "    # scheduler = get_linear_schedule_with_warmup(\n",
        "    #   optimizer,\n",
        "    #   num_warmup_steps=self.n_warmup_steps,\n",
        "    #  num_training_steps = -1\n",
        "    # )\n",
        "\n",
        "    return dict(\n",
        "      optimizer=optimizer,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class for loading the data\n"
      ],
      "metadata": {
        "id": "HLr7AKPmsIJf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uqk9jTWBnAus"
      },
      "outputs": [],
      "source": [
        "\n",
        "class InferenceDataModue(pl.LightningDataModule):\n",
        "  '''\n",
        "  Module for Data loading. This module bind the text data and tokenizer on them.\n",
        "  test_df(dataframe): having column cleaned_text and class \n",
        "  batchsize(int): 1 for prediction\n",
        "  max_token_len(int): Maximum length of token for tokenizer\n",
        "  '''\n",
        "  def __init__(self,test_df, tokenizer, batch_size=1, max_token_len=64):\n",
        "    super().__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.test_df = test_df\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_token_len = max_token_len\n",
        "\n",
        "\n",
        "    self.test_dataset = CallCenterDataset(\n",
        "      self.test_df,\n",
        "      self.tokenizer,\n",
        "      self.max_token_len)\n",
        "    self.predict_dataset = CallCenterDataset(\n",
        "      self.test_df,\n",
        "      self.tokenizer,\n",
        "      self.max_token_len)\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(\n",
        "      self.test_dataset,\n",
        "      batch_size=self.batch_size,\n",
        "      num_workers=0\n",
        "    )\n",
        "  def predict_dataloader(self):\n",
        "    return DataLoader(\n",
        "      self.test_dataset,\n",
        "      batch_size=self.batch_size,\n",
        "      num_workers=0\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IdrakTinyBertInference** class is the main class, where we are loading the **trained-model from checkpoint** and predicting over **text**.\n",
        "\n"
      ],
      "metadata": {
        "id": "uzMeuUG_si8N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeBnmNPopGNj"
      },
      "outputs": [],
      "source": [
        "class IdrakTinyBertInference:\n",
        "  def __init__(self,datapath='',model_path='prajjwal1/bert-tiny',batch_size=1,classifier=1,dataset=1,model_name='',drive_folder='/content/gdrive/MyDrive/idraak/model/tinybert_report',checkpoint_name='best_checkpoint'):\n",
        "    '''\n",
        "    model_path(str): the path of tiny bert\n",
        "    batch_size(int): 1 for prediction \n",
        "    classifier(int): classifier label\n",
        "    dataset(int): dataset label\n",
        "    drive_folder(str): the path of pretrained checkpoint\n",
        "    '''\n",
        "    self.df_test=pd.DataFrame()\n",
        "    self.text=''\n",
        "    self.classifier=classifier\n",
        "    self.dataset=dataset\n",
        "    self.drive_folder=drive_folder\n",
        "    #Class Maps for Class value to Labels\n",
        "    self.classifier_1_labelmaps={0:'answering_machine',1:'dnc'}\n",
        "    self.classifier_2_labelmaps={0:'answering_machine',1:'busy',2:'dnc',3:'greetback',4:'greeting',5:'other',6:'sorry_greeting',7:'spanish'}\n",
        "    self.classifier_3_labelmaps={0:'bot',1:'busy',2:'dnc',3:'negative',4:'not_intrested',5:'other',6:'positive',7:'spanish'}\n",
        "    self.classifier_4_labelmaps={0:'dnc',1:'negative',2:'not_intrested',3:'other',4:'positive'}\n",
        "    self.classifier_5_labelmaps={0:'dnc',1:'negative',2:'not_intrested',3:'other',4:'positive'}\n",
        "    self.classifiers_meta={1:{'labels':self.classifier_1_labelmaps,'NUM_CLASSES':2},\n",
        "                  2:{'labels':self.classifier_2_labelmaps,'NUM_CLASSES':8},\n",
        "                  3:{'labels':self.classifier_3_labelmaps,'NUM_CLASSES':8},\n",
        "                  4:{'labels':self.classifier_4_labelmaps,'NUM_CLASSES':5},\n",
        "                  5:{'labels':self.classifier_5_labelmaps,'NUM_CLASSES':5}\n",
        "                  }\n",
        "    #Generated Text Point Path from classifier and dataset value passed\n",
        "    self.checkpoint_path='{}/best_dataset{}_classifier_{}.ckpt'.format(self.drive_folder,self.dataset,self.classifier)\n",
        "    self.checkpoint_name='best_'+checkpoint_name\n",
        "    self.LABEL_COLUMNS=self.classifiers_meta[classifier]['NUM_CLASSES']\n",
        "    self.log_dir = \"lightning_logs/IDRAK/version_0\"\n",
        "    self.drive_folder=drive_folder\n",
        "    self.model_name=model_name\n",
        "    self.model_path=model_path\n",
        "    self.MAX_TOKEN_COUNT=71\n",
        "    self.BATCH_SIZE=batch_size\n",
        "    #Defining Bert Tokenizer\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    self.model = CallCenterTagger(n_classes=self.LABEL_COLUMNS,model_path=self.model_path,n_warmup_steps=0,n_training_steps=-1,learning_rate=0)\n",
        "    self.model=self.model.load_from_checkpoint(self.checkpoint_path,model_path=self.model_path,n_classes=self.LABEL_COLUMNS)\n",
        "    self.save_model_name=''\n",
        "    self.checkpoint_callback = ModelCheckpoint(dirpath=\"checkpoints\",filename=self.checkpoint_name,save_top_k=1,verbose=True,monitor=\"val_loss\",mode=\"min\")\n",
        "    self.logger = TensorBoardLogger(\"lightning_logs\", name=\"IDRAK\")\n",
        "    self.bar = LitProgressBar()\n",
        "    self.early_stopping_callback = EarlyStopping(monitor='val_loss', patience=1)\n",
        "    self.trainer = pl.Trainer(logger=self.logger,callbacks=[self.early_stopping_callback,self.checkpoint_callback,self.bar],max_epochs=0)\n",
        "    self.dm=InferenceDataModue(test_df=self.df_test,tokenizer=self.tokenizer,batch_size=1,max_token_len=self.MAX_TOKEN_COUNT)\n",
        "    \n",
        "  def predict(self,text):\n",
        "    '''\n",
        "    prediction function. This function get a text string from Infernce object and return the \n",
        "    predicted class, probibilities and class labels using trianed pytorch model\n",
        "    '''\n",
        "    self.text=text\n",
        "    prediction=self.prep_data()\n",
        "    prediction=prediction[0].cpu().detach().numpy()\n",
        "    y_pred=prediction[0].argmax()\n",
        "    prob=softmax(prediction)[0]\n",
        "    class_label=self.classifiers_meta[self.classifier]['labels'][y_pred]\n",
        "    result={'prob':prob,'class':y_pred,'class_label':class_label}\n",
        "    return result\n",
        "  def prep_data(self):\n",
        "    #function to make dataframe\n",
        "    self.text=self.cleanify()\n",
        "    self.df_test['cleaned_text']=[self.text]\n",
        "    self.df_test['class']=[1] #dummy label\n",
        "    self.df_test['class_labels']=['xx']\n",
        "    # print(self.df_test)\n",
        "    self.dm= self.dm=InferenceDataModue(test_df=self.df_test,tokenizer=self.tokenizer,batch_size=1,max_token_len=self.MAX_TOKEN_COUNT)\n",
        "    p=self.trainer.predict(self.model,datamodule=self.dm)\n",
        "    return p\n",
        "  def eval_model(self):\n",
        "    pass\n",
        "  def cleanify(self):\n",
        "    #function to clean text\n",
        "    '''\n",
        "    This is inner function. It will first remove the unwantted symbols from text\n",
        "    using regular expression. Then Keep the numbers, alphabets, and question mark \n",
        "    '''\n",
        "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]') #compile regulare expression for removing symbols\n",
        "    BAD_SYMBOLS_RE = re.compile('[^0-9a-z ?]') #compile regulare expression to keep wanted data\n",
        "    text=str(self.text)\n",
        "    text = text.lower() #making text to lower case\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)  #applying 1 and 2nd mentioned re\n",
        "    text = BAD_SYMBOLS_RE.sub(' ', text)\n",
        "    return text\n",
        "  def __repr__(self):\n",
        "    return 'IdrakTinyBertInference(num_class={},trained_model_path={})'.format(self.LABEL_COLUMNS, self.checkpoint_path)    \n",
        "  def __str__(self):\n",
        "    return 'IdrakTinyBertInference Trained over {}'.format(self.checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier=3"
      ],
      "metadata": {
        "id": "w88lkCOJtW5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=3"
      ],
      "metadata": {
        "id": "vfWOox0DuDZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dirpath='gdrive/MyDrive/idraak/model/tinybert_report'"
      ],
      "metadata": {
        "id": "6MJRVzg2uVne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The constructor of IdrakTinyBertInfrence Requires 3 parameters.\n",
        "\n",
        "1: `classifier`  expected input any number between 1 to 5. (This para donated the classifier intro, hello etc\n",
        "\n",
        "2: `dataset` expected input any number between1 to 4 \n",
        "This para about the dataset on which model is trained. \n",
        "\n",
        "3: `drive_folder` path of drive where trained models are stored."
      ],
      "metadata": {
        "id": "kwsuMNxrv2FI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "itbf=IdrakTinyBertInference(classifier=5,dataset=2,drive_folder=model_dirpath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wF9bwxUlNO5",
        "outputId": "f05c0bd6-bb14-4aaf-be86-2ba8bf91e4d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For prediction you need to pass a text string to IdrakTinyBertInference object `predict` function as\n",
        "```\n",
        "result=itbf.predict(text)\n",
        "```"
      ],
      "metadata": {
        "id": "IO-xOuocw0js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text='random Text'"
      ],
      "metadata": {
        "id": "hVbawGwDnjTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=itbf.predict(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHPOeDjbVx2L",
        "outputId": "374600d7-8ac7-45bc-a244-72e0d54b87ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: lightning_logs/IDRAK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am called\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Output:**\n",
        "\n",
        "`prob` : probabilities of each class\n",
        "\n",
        "`class`: class numaric label\n",
        "\n",
        "`class_label`: class label in human readable format"
      ],
      "metadata": {
        "id": "J88pxYXdxK0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0rRIySAu_Wt",
        "outputId": "04fdf2a3-1808-4ab5-d604-b94a26903a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prob': array([0.15899259, 0.05700221, 0.12287708, 0.63530654, 0.02582158],\n",
              "       dtype=float32), 'class': 3, 'class_label': 'other'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks"
      ],
      "metadata": {
        "id": "ULG92zPsycR-"
      }
    }
  ]
}